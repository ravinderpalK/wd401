
# Docker

Docker is a set of platform as a service (PaaS) products that uses OS-level virtualization to deliver software in packages called containers. Containers are isolated from each other and bundle their own software, libraries, and configuration files.

## How Docker Works:

1. Building Images: Developers create Docker images using Dockerfiles, which define the environment and dependencies needed for the application. Docker builds the images using the instructions specified in the Dockerfile.
2. Running Containers: Developers can run Docker containers using the Docker CLI. Containers are isolated instances of an image that run as a process on the host machine. Multiple containers can run simultaneously on the same host.
3. Container Orchestration: Docker Swarm and Kubernetes are popular tools for orchestrating and managing Docker containers in production environments. They automate the deployment, scaling, and management of containerized applications.

### Dockerfile
```
FROM --platform=$BUILDPLATFORM node:lts-alpine as base
WORKDIR /app
COPY package.json /
EXPOSE 3000

FROM base as production
ENV NODE_ENV=production
RUN npm i -g husky
RUN npm install
COPY . /app
CMD node index.js

FROM base as dev
ENV NODE_ENV=devlopment
RUN npm install -g nodemon && npm install
COPY . /app
CMD npm run start
```
This Dockerfile is used to build a Docker image for a Node.js application. It utilizes multi-stage builds to create separate stages for production and development environments
### Base
  1. FROM --platform=$BUILDPLATFORM node:lts-alpine as base: This line sets the base image for the build stage, using the Node.js LTS Alpine image.
  2. The --platform=$BUILDPLATFORM flag ensures compatibility with the platform where the image will be built.
  3. WORKDIR /app: Sets the working directory inside the container to /app.
  4. COPY package.json /: Copies the package.json file from the host to the root directory of the container.
  5. EXPOSE 3000: Exposes port 3000, which is commonly used for Node.js applications.

### Production
  1. FROM base as production: This line indicates that this stage extends the base stage defined earlier. It ensures that all the dependencies and environment variables from the base stage are inherited.
  2. ENV NODE_ENV=production: Sets the NODE_ENV environment variable to production, indicating that this stage is for production.
  3. RUN npm i -g husky: Installs the Husky package globally. Husky is typically used for setting up Git hooks.
  4. RUN npm install: Installs the dependencies listed in package.json.
  5. COPY . /app: Copies the rest of the application files from the host to the /app directory in the container.
  6. CMD node index.js: Specifies the command to run when the container starts. In this case, it runs the index.js file using Node.js.

### Dev
  1. FROM base as dev: This line indicates that this stage extends the base stage defined earlier.
  2. ENV NODE_ENV=development: Sets the NODE_ENV environment variable to development, indicating that this stage is for development.
  3. RUN npm install -g nodemon && npm install: Installs the Nodemon package globally and installs project dependencies. Nodemon is a tool that monitors changes in the source code and automatically restarts the Node.js application.
  4. COPY . /app: Copies the rest of the application files from the host to the /app directory in the container.
  5. CMD npm run start: Specifies the command to run when the container starts. In this case, it runs the start script defined in package.json.


## Docker compose for production
```
version: "3.8"
services:
  app:
    build:
      context: .
      target: production
    image: todo-app:production
    volumes:
      - .:/app
    ports:
      - 3100:3000
    env_file:
      - .env
    depends_on:
      - db

  db:
    image: postgres:15
    volumes:
      - pg-dev-data:/var/lib/postgresql/data
    env_file:
      - .env
    environment:
      POSTGRES_USER: $DEV_USERNAME
      POSTGRES_DB: $DEV_DATABASE
      POSTGRES_PASSWORD: $DEV_PASSWORD

volumes:
  pg-dev-data:

```
The Docker Compose file for production is designed to set up your application in a production environment where stability, scalability, and performance are critical.

## Docker compose for development
```
FROM --platform=$BUILDPLATFORM node:lts-alpine as base
WORKDIR /app
COPY package.json /
EXPOSE 3000

FROM base as production
ENV NODE_ENV=production
RUN npm i -g husky
RUN npm install pm2 -g
RUN npm install
COPY . /app
CMD node index.js

FROM base as dev
ENV NODE_ENV=devlopment
RUN npm install pm2 -g
RUN npm install -g nodemon && npm install
COPY . /app
CMD npm run start
```
The Docker Compose file for development is tailored to streamline the development workflow and provide developers with a consistent and reproducible environment for testing and debugging.

Separating Docker Compose files for production and development environments allows you to maintain a clear distinction between configurations tailored for each environment. This helps in ensuring consistency, stability, and efficiency across different stages of the application lifecycle, from development to production deployment. Additionally, it provides flexibility for developers to iterate and experiment in a controlled and reproducible environment during the development process.

## Advantages of Docker:

1. Portability: Docker containers can run on any platform that supports Docker, providing consistent behavior across different environments.
2. Isolation: Containers are isolated from each other and the host system, ensuring that changes made to one container do not affect others.
3. Efficiency: Containers are lightweight and share the host OS kernel, leading to faster startup times and lower resource usage compared to virtual machines.
4. Consistency: Docker images define the environment and dependencies needed for an application, ensuring consistent behavior across development, testing, and production environments.
5. Scalability: Docker containers can be easily scaled up or down to handle varying workloads, making them ideal for microservices architectures.



# Configure Environment Variables in Docker

1. Use Secrets for Sensitive Information: Docker provides a mechanism called secrets to manage sensitive data like passwords securely. We'll utilize this feature to manage database credentials.
2. Separate Environment Variable Files: It's a good practice to separate environment variables for different environments into different files. We'll create .env files for both production and development environments.
3. Update Docker Compose to Use Secrets: We'll update the Docker Compose file to use secrets for the database credentials.

### env file
```
# PRODUCTION
NODE_ENV=production
PROD_USERNAME=postgres
PROD_PASSWORD=ravi
PROD_DATABASE=todo
PROD_HOST=db
PROD_DIALECT=postgres

# DEVLOPMENT
DEV_USERNAME=postgres
DEV_PASSWORD=ravi
DEV_DATABASE=todo
DEV_HOST=db
DEV_DIALECT=postgres

DATABASE_URL=postgres://postgres:ravi@db:5432/wd-todo-dev
```

We've separated the environment variables for production and development into different .env files.
The Docker Compose file now references .env for both the app and db services.
All the values like username, password, database_name are stored in a seprate env file.

## Advantage of seprate env file

1. Isolation of Configuration: By having separate environment variable files, you can isolate the configuration specific to each environment. This separation makes it easier to manage and update configurations without affecting other environments.

2. Security: Environment variable files can contain sensitive information like passwords, API keys, and other secrets. Separating these variables into environment-specific files allows you to control access to sensitive information more effectively. For instance, you might have different access controls or encryption mechanisms for production environment files compared to development environment files.

3. Clarity and Readability: Having separate files improves clarity and readability of your Docker Compose configuration. It's clear which variables are intended for each environment, making it easier for developers to understand and maintain the setup.

4. Flexibility: Different environments may require different configurations. For example, in development, you might want verbose logging and debugging features enabled, while in production, you may want optimizations for performance and security. Separate environment files allow you to customize configurations according to the specific needs of each environment.

5. Consistency: Using separate environment files promotes consistency across different environments. Developers can rely on a consistent structure and naming convention for environment variables, reducing the likelihood of configuration errors.

# CI/CD Pipeline

```
name: CI/CD
# Triggers the workflow on every push events
on: push

# Environment variables

env:
  PG_DATABASE: "${{ secrets.POSTGRES_DATABASE }}"
  PG_USER: "${{ secrets.POSTGRES_USER }}"
  PG_PASSWORD: "${{ secrets.POSTGRES_PASSWORD }}"

# Jobs
jobs:
  # Job to run tests
  run-tests:
    # Runs on Ubuntu latest version
    runs-on: ubuntu-latest
    # Define a PostgreSQL service for running tests
    services:
      postgres:
        image: postgres:11.7
        env:
          POSTGRES_USER: "postgres"
          POSTGRES_PASSWORD: "ravi"
          POSTGRES_DB: "wd-todo-test"
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    # Steps to execute within the job
    steps:
      # Check out repository code
      - name: Check out repository code
        uses: actions/checkout@v3

        # Install dependencies
      - name: Install dependencies
        run: cd todo-app && npm ci
      # Run unit tests
      - name: Run unit tests
        run: cd todo-app && npm test
      # Run the app
      - name: Run the app
        id: run-app
        run: |
          cd todo-app && npx sequelize-cli db:drop
          npx sequelize-cli db:create
          npx sequelize-cli db:migrate
          PORT=3000 npm start &
          sleep 5
      # Run integration tests
      - name: Run integration tests
        run: |
          cd todo-app && npm install cypress cypress-json-results
          npx cypress run

  docker:
    needs: run-tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Build Docker image
        run: |
          cd todo-app && docker build -f Dockerfile --target production -t raviinderpalsingh/todo-app-prod:latest .
      - name: Push Docker image to Docker Hub
        run: |
          cd todo-app && docker push raviinderpalsingh/todo-app-prod:latest

  # Job to deploy the app to production
  deploy:
    # Define the job dependencies
    needs: [run-tests,docker]
    runs-on: ubuntu-latest
    if: needs.run-tests.result == 'success'

    # Steps to execute within the job
    steps:
      # Deploy to production using a custom action
      - name: Deploy to production
        uses: johnbeynon/render-deploy-action@v0.0.8
        with:
          service-id: "${{ secrets.MY_RENDER_SERVICE_ID }}"
          api-key: "${{ secrets.MY_RENDER_API_KEY }}"

  # Job to send Slack notifications
  notify:
    # Define the job dependencies
    needs: [run-tests, deploy]
    runs-on: ubuntu-latest
    if: ${{ always() }}
    steps:
      - name: Send Slack notification on success

        # Send a Slack notification if the tests and deployment are successful
        if: ${{ needs.run-tests.result == 'success' && needs.deploy.result == 'success' }}
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "CI/CD process succeeded!" 
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    
      - name: Send Slack notification on failure
        if: ${{ needs.run-tests.result != 'success' || needs.deploy.result != 'success' }}
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "*${{ github.workflow }}* failed. Access the details https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}."
               
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
```

The workflow is triggered on every push event to the main branch.
Environment variables are defined to securely store sensitive information such as PostgreSQL credentials and Docker Hub credentials.

### Jobs
Run Tests:
  1. Runs on an Ubuntu latest version.
  2. Uses a PostgreSQL service for running tests.
  3. Checks out repository code.
  4. Installs dependencies and runs unit tests.
  5. Sets up and runs the application, performs database migrations, and runs integration tests using Cypress.

Docker:
  1. Depends on the run-tests job.
  2. Runs on an Ubuntu latest version.
  3. Builds the Docker image for production using the Dockerfile.
  4. Pushes the Docker image to Docker Hub.

Deploy:
  1. Depends on the run-tests and docker jobs.
  2. Runs on an Ubuntu latest version.
  3. Executes the deployment to production using a custom action.
  4. The deployment action is configured with Render service ID and API key stored in GitHub Secrets.

Notify:
  1. Depends on the run-tests and deploy jobs.
  2. Runs on an Ubuntu latest version.
  3. Sends Slack notifications based on the outcome of the previous jobs.
  4. Sends a success message if all previous jobs are successful and a failure message otherwise.

This GitHub Actions workflow provides a comprehensive CI/CD pipeline for automating the testing, building, deployment, and notification processes. It ensures that changes to the codebase are thoroughly tested, packaged into Docker images, deployed to production environments, and notifies relevant stakeholders about the outcome of the pipeline.